# ğŸŒ¤ï¸ Daily Weather Monitor â€“ Airflow Pipeline

This project implements an **Apache Airflow** data pipeline that monitors London's weather conditions using the **Open-Meteo API**. The pipeline extracts hourly weather data, summarizes daily statistics, checks for extreme conditions, and loads the results into a **MySQL** database for long-term storage and analysis.

---

## ğŸ“ Project Structure

project-root/
â”œâ”€â”€ config/ # Configuration files (optional, e.g., .env)
â”œâ”€â”€ dags/
â”‚ â””â”€â”€ daily_weather_dag.py # ğŸŒ€ Main Airflow DAG with the full pipeline
â”œâ”€â”€ logs/ # Airflow task logs
â”œâ”€â”€ docker-compose.yaml # Docker setup for Airflow services
â””â”€â”€ .DS_Store # macOS system file (can be ignored)


> âœ… All DAG logic is contained in `dags/daily_weather_dag.py`.

---

## ğŸš€ Features

- â° **Hourly API requests** to Open-Meteo for current weather in London.
- ğŸ“¦ **CSV-based staging** of hourly and daily summaries inside the container.
- ğŸ“Š **Daily summaries**: min/max temperatures, average wind speed, and rain detection.
- ğŸš¨ **Extreme weather alerts**: checks for high temperatures, rain, snow, and storms.
- ğŸ—ƒï¸ **Data persistence**: Loads clean weather data into MySQL tables.
- ğŸ” **Runs hourly** with a built-in scheduler (Airflow).
- ğŸ³ **Docker-compatible** for easy orchestration.

---

## ğŸ› ï¸ Prerequisites

- Python 3.8+
- Docker + Docker Compose
- Apache Airflow 2.7+ (via Docker or manual)
- MySQL server with a database named `apache_weather_data`
- Python packages:
  - `requests`
  - `pandas`
  - `sqlalchemy`
  - `mysql-connector-python`
  - `python-dotenv`

---

## âš™ï¸ Setup Instructions

1. **Clone the repository**

   ```bash
   git clone https://github.com/your-username/weather-airflow-pipeline.git
   cd weather-airflow-pipeline

2. (Optional) Install Python dependencies locally
pip install -r requirements.txt

3. Start Airflow using Docker Compose
docker-compose up airflow-init   # Initialize metadata DB
docker-compose up                # Launch webserver, scheduler, etc.

4. Access the Airflow web UI
Open http://localhost:8080
Default credentials: airflow / airflow

5. Trigger the DAG
DAG ID: simple_weather_to_csv
Either wait for hourly scheduling or trigger manually from the UI.


â›“ï¸ DAG Workflow Overview
The DAG defined in daily_weather_dag.py follows this task sequence:

is_api_available
       â†“
extract_transform_weather
       â†“
summarize_weather
       â†“
check_extreme_weather
       â†“
load_to_mysql

6. 
| Task ID                     | Purpose                                               |
| --------------------------- | ----------------------------------------------------- |
| `is_api_available`          | Verifies Open-Meteo API availability                  |
| `extract_transform_weather` | Fetches current data and appends it to hourly CSV     |
| `summarize_weather`         | Creates daily metrics (min/max temp, avg wind, rain)  |
| `check_extreme_weather`     | Detects rain, snow, high temps, and storms from codes |
| `load_to_mysql`             | Loads hourly and summary data into MySQL database     |


ğŸ—ƒï¸ MySQL Database Schema
You need to create a MySQL database named apache_weather_data with two tables:

weather_hourly
date, time, temperature, relative_humidity, wind_speed, rain, precipitation, weather_code, alerts
weather_summary
date, min_temp, max_temp, avg_wind_speed, rain_expected
Note: Tables are created automatically if they don't exist (via SQLAlchemy).

ğŸ“Œ New in This Version
âœ… Added 3 DAG tasks:
summarize_weather
check_extreme_weather
load_to_mysql
âœ… Complete inline alerting logic
âœ… Local file persistence in /tmp
âœ… Enhanced logging and error handling

ğŸ”® Future Improvements
Email/Slack alerts for extreme weather
Configurable cities (via Airflow Variables)
Dashboard integration (Streamlit, Metabase, etc.)
CSV cleanup and rotation policy
Historical trend analysis
Support for Parquet files or cloud storage

